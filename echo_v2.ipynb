{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: /Users/sali/Library/Application Support/crewai_storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sali/miniconda3/envs/ML/lib/python3.10/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
      "* 'fields' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: /Users/sali/Library/Application Support/crewai_storage\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from crewai import LLM\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = LLM(\n",
    "    model=os.getenv(\"FIREWORKS_MODEL_NAME\"),\n",
    "    base_url=\"https://api.fireworks.ai/inference/v1\",\n",
    "    api_key=os.getenv(\"FIREWORKS_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from echo.runner import make_call\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "test_clients = [\n",
    "\t\"Shell\",\n",
    "\t\"Schneider electric\"\n",
    "]\n",
    "\n",
    "\n",
    "train_clients = [\n",
    "\t\"ICICI bank\",\n",
    "\t\"Infosys\",\t\n",
    "\t\"University of Illinois\",\n",
    "\t\"Marks and spencer\",\n",
    "\t\"Mercedes Benz\",\n",
    "]\n",
    "\n",
    "clients = test_clients + train_clients\n",
    "\n",
    "\n",
    "inputs = {\n",
    "    'call_type': 'discovery', \n",
    "    'seller': 'Whatfix',\n",
    "    'n_competitors': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discovery_calls_data = asyncio.run(make_call('discovery', clients, inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object aget_simulation_data_for_client at 0x3125c27a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from echo.step_templates.discovery import aget_simulation_data_for_client\n",
    "\n",
    "simulation_data = await aget_simulation_data_for_client(inputs, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_calls_data = asyncio.run(make_call('demo', clients, inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pricing_call_data = asyncio.run(make_call('pricing', clients, inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negotiation_call_data = asyncio.run(make_call('negotiation', clients, inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory Types\n",
    "\n",
    "- Deal Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.vector_stores import (\n",
    "    MetadataFilter,\n",
    "    MetadataFilters,\n",
    "    FilterOperator,\n",
    "    FilterCondition\n",
    ")\n",
    "\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "import chromadb\n",
    "\n",
    "\n",
    "nodes = [\n",
    "    TextNode(\n",
    "        text=\"The Shawshank Redemption\",\n",
    "        metadata={\n",
    "            \"author\": \"Stephen King\",\n",
    "            \"theme\": \"Friendship\",\n",
    "            \"year\": 1994,\n",
    "        },\n",
    "    ),\n",
    "    TextNode(\n",
    "        text=\"The Godfather\",\n",
    "        metadata={\n",
    "            \"director\": \"Francis Ford Coppola\",\n",
    "            \"theme\": \"Mafia\",\n",
    "            \"year\": 1972,\n",
    "        },\n",
    "    ),\n",
    "    TextNode(\n",
    "        text=\"Inception\",\n",
    "        metadata={\n",
    "            \"director\": \"Christopher Nolan\",\n",
    "            \"theme\": \"Fiction\",\n",
    "            \"year\": 2010,\n",
    "        },\n",
    "    ),\n",
    "    TextNode(\n",
    "        text=\"To Kill a Mockingbird\",\n",
    "        metadata={\n",
    "            \"author\": \"Harper Lee\",\n",
    "            \"theme\": \"Mafia\",\n",
    "            \"year\": 1960,\n",
    "        },\n",
    "    ),\n",
    "    TextNode(\n",
    "        text=\"1984\",\n",
    "        metadata={\n",
    "            \"author\": \"George Orwell\",\n",
    "            \"theme\": \"Totalitarianism\",\n",
    "            \"year\": 1949,\n",
    "        },\n",
    "    ),\n",
    "    TextNode(\n",
    "        text=\"The Great Gatsby\",\n",
    "        metadata={\n",
    "            \"author\": \"F. Scott Fitzgerald\",\n",
    "            \"theme\": \"The American Dream\",\n",
    "            \"year\": 1925,\n",
    "        },\n",
    "    ),\n",
    "    TextNode(\n",
    "        text=\"Harry Potter and the Sorcerer's Stone\",\n",
    "        metadata={\n",
    "            \"author\": \"J.K. Rowling\",\n",
    "            \"theme\": \"Fiction\",\n",
    "            \"year\": 1997,\n",
    "        },\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "from typing import Dict\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from echo.utils import db_storage_path\n",
    "\n",
    "CHUNK_SIZE=8192\n",
    "CHUNK_OVERLAP=128\n",
    "SIMILARITY_TOP_K=3\n",
    "\n",
    "\n",
    "class IndexType(enum.Enum):\n",
    "    HISTORICAL = \"historical\"\n",
    "    CURRENT_DEAL = \"current_deal\"\n",
    "    BUYER_RESEARCH = \"buyer_research\"\n",
    "    SELLER_RESEARCH = \"seller_research\"\n",
    "    SALES_PLAYBOOK = \"sales_playbook\"\n",
    "\n",
    "\n",
    "class CallType(enum.Enum):\n",
    "    DISCOVERY = \"discovery\"\n",
    "    DEMO = \"demo\"\n",
    "    PRICING = \"pricing\"\n",
    "    PROCUREMENT = \"procurement\"\n",
    "\n",
    "\n",
    "def get_vector_index(\n",
    "    index_name: str, \n",
    "    index_type: IndexType\n",
    "):\n",
    "    chroma_db_path = db_storage_path(index_name)\n",
    "    db = chromadb.PersistentClient(path=str(chroma_db_path))\n",
    "    chroma_collection = db.get_or_create_collection(f\"{index_type.value}\")\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    index = VectorStoreIndex.from_vector_store(vector_store)\n",
    "\n",
    "    return index\n",
    "\n",
    "\n",
    "def get_response(\n",
    "    query: str, \n",
    "    index: VectorStoreIndex, \n",
    "    filters: Dict[str, str] = None, \n",
    "    filter_operator: FilterOperator = FilterOperator.EQ,\n",
    "    condition: FilterCondition = FilterCondition.AND,\n",
    "    \n",
    "):\n",
    "    filters = filters or {}\n",
    "    filters = MetadataFilters(\n",
    "        filters=[\n",
    "            MetadataFilter(\n",
    "                key=k.lower(), \n",
    "                value=v.lower(),\n",
    "                operator=filter_operator\n",
    "            ) for k, v in filters.items()\n",
    "        ],\n",
    "        condition=condition,\n",
    "    )\n",
    "\n",
    "    return index.as_query_engine(filters=filters).query(query)\n",
    "\n",
    "\n",
    "def get_nodes_from_documents(data: str, metadata: Dict[str, str]):\n",
    "    splitter = SentenceSplitter(\n",
    "        chunk_size=CHUNK_SIZE, \n",
    "        chunk_overlap=CHUNK_OVERLAP\n",
    "    )\n",
    "    docs = splitter.split_text(data)\n",
    "    return [\n",
    "        TextNode(\n",
    "            text=doc,\n",
    "            metadata=metadata\n",
    "        )\n",
    "        for doc in docs\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_keys = {\n",
    "    IndexType.HISTORICAL: [\n",
    "        \"seller\", \n",
    "        \"buyer\", \n",
    "        \"call_type\", \n",
    "        \"company_size\", \n",
    "        \"industry\", \n",
    "        \"description\"\n",
    "    ],\n",
    "    IndexType.BUYER_RESEARCH: [\"buyer\"],\n",
    "    IndexType.SELLER_RESEARCH: [\"seller\"],\n",
    "    IndexType.SALES_PLAYBOOK: [],\n",
    "}\n",
    "\n",
    "def add_data(\n",
    "    data: str, \n",
    "    metadata: Dict[str, str], \n",
    "    index_name: str, \n",
    "    index_type: IndexType\n",
    "):\n",
    "    assert all([k in metadata for k in metadata_keys[index_type]]), f\"Missing metadata keys for {index_type}. \\nRequired keys: {metadata_keys[index_type]}. \\nProvided keys: {metadata.keys()}\"\n",
    "    filtered_metadata = {\n",
    "        k.lower(): v.lower() for k, v in metadata.items() \n",
    "        if k in metadata_keys[index_type]\n",
    "    }\n",
    "    index = get_vector_index(index_name, index_type)\n",
    "    nodes = get_nodes_from_documents(data, metadata=filtered_metadata)\n",
    "    index.insert_nodes(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: /Users/sali/Library/Application Support/crewai_storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sali/miniconda3/envs/ML/lib/python3.10/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
      "* 'fields' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: /Users/sali/Library/Application Support/crewai_storage\n"
     ]
    }
   ],
   "source": [
    "from echo.indexing import IndexType, get_vector_index\n",
    "from typing import Type\n",
    "from crewai.tools import BaseTool\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from llama_index.core.vector_stores import (\n",
    "    MetadataFilter,\n",
    "    MetadataFilters\n",
    ")\n",
    "\n",
    "from echo.settings import (\n",
    "    SIMILARITY_TOP_K\n",
    ")\n",
    "\n",
    "\n",
    "class HistoricalIndexArguments(BaseModel):\n",
    "    \"\"\"Input schema for MyCustomTool.\"\"\"\n",
    "    query: str = Field(..., description=\"Query to retrieve from the index.\")\n",
    "    call_type: str = Field(..., description=\"Type of call\")\n",
    "    buyer: str = Field(..., description=\"Name of the buyer.\")\n",
    "    seller: str = Field(..., description=\"Name of the seller.\")\n",
    "\n",
    "\n",
    "class HistoricalCallIndex(BaseTool):\n",
    "    name: str = \"Historical Calls Vector Index\"\n",
    "    description: str = (\n",
    "        \"This tool retrieves relevant the historical calls data from the vector index for a given deal.\"\n",
    "        \"This tool uses buyer, seller and type to retrieve the relevant calls.\"\n",
    "        \"And then uses the query to retrieve the most relevant calls.\"\n",
    "        \"This tool can be used to see what has worked in the past for similar deals.\"\n",
    "    )\n",
    "    args_schema: Type[BaseModel] = HistoricalIndexArguments\n",
    "\n",
    "    def _run(self, query: str, call_type: str, buyer: str, seller: str) -> str:\n",
    "        index = get_vector_index(seller, IndexType.HISTORICAL)\n",
    "        \n",
    "        filters_dict = {\n",
    "            \"seller\": seller,\n",
    "            \"buyer\": buyer,\n",
    "            \"call_type\": call_type\n",
    "        }\n",
    "\n",
    "        filters = MetadataFilters(\n",
    "            filters=[\n",
    "                MetadataFilter(\n",
    "                    key=k, \n",
    "                    value=v,\n",
    "                ) for k, v in filters_dict.items()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        retrieved_nodes = index.as_retriever(filters=filters, similarity_top_k=SIMILARITY_TOP_K).retrieve(query)\n",
    "        return f\"\\n\".join([f\"Historical Call: {i}\\n{n.get_content()}\" for i, n in enumerate(retrieved_nodes)])\n",
    "\n",
    "\n",
    "class CurrentCallIndexArguments(BaseModel):\n",
    "    \"\"\"Input schema for MyCustomTool.\"\"\"\n",
    "    call_type: str = Field(..., description=\"Type of call\")\n",
    "    buyer: str = Field(..., description=\"Name of the buyer.\")\n",
    "    seller: str = Field(..., description=\"Name of the seller.\")\n",
    "\n",
    "\n",
    "class CurrentCallIndex(BaseTool):\n",
    "    name: str = \"Historical Calls Vector Index\"\n",
    "    description: str = (\n",
    "        \"This tool retrieves relevant documents related to the current call.\"\n",
    "        \"This tool uses buyer, seller and type to retrieve the relevant calls to retrieve the relevant call records.\"\n",
    "    )\n",
    "    args_schema: Type[BaseModel] = CurrentCallIndexArguments\n",
    "\n",
    "    def _run(self, call_type: str, buyer: str, seller: str) -> str:\n",
    "        index = get_vector_index(seller, IndexType.HISTORICAL)\n",
    "        \n",
    "        filters_dict = {\n",
    "            \"seller\": seller,\n",
    "            \"buyer\": buyer,\n",
    "            \"call_type\": call_type\n",
    "        }\n",
    "\n",
    "        filters = MetadataFilters(\n",
    "            filters=[\n",
    "                MetadataFilter(\n",
    "                    key=k, \n",
    "                    value=v,\n",
    "                ) for k, v in filters_dict.items()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        retrieved_nodes = index.as_retriever(filters=filters, similarity_top_k=SIMILARITY_TOP_K).retrieve(\"\")\n",
    "        return f\"\\n\".join([f\"Current {call_type} Call Data: {i}\\n{n.get_content()}\" for i, n in enumerate(retrieved_nodes)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
